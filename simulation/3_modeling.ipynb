{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import training\n",
    "import config\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import MyTransformer\n",
    "\n",
    "models = ['gpt-nano', 'gpt-micro', 'gpt-mini', 'gopher-44m', 'gpt2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment setup is model_0_lr_-4_seed_0.\n",
      "number of parameters: 0.10M\n",
      "The ID for this training is -1269400444507779122_1693257366.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/haibinzhao/Desktop/TECO/17_SNN/Code/simulation/3_modeling.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haibinzhao/Desktop/TECO/17_SNN/Code/simulation/3_modeling.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m lossfunction \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haibinzhao/Desktop/TECO/17_SNN/Code/simulation/3_modeling.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlr)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/haibinzhao/Desktop/TECO/17_SNN/Code/simulation/3_modeling.ipynb#W6sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m model, train_loss, valid_loss \u001b[39m=\u001b[39m training\u001b[39m.\u001b[39;49mtrain_nn(model, train_loader, valid_loader, lossfunction, optimizer, UUID\u001b[39m=\u001b[39;49mexp_setup)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haibinzhao/Desktop/TECO/17_SNN/Code/simulation/3_modeling.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./NNs/predictor_\u001b[39m\u001b[39m{\u001b[39;00mexp_setup\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haibinzhao/Desktop/TECO/17_SNN/Code/simulation/3_modeling.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n",
      "File \u001b[0;32m~/Desktop/TECO/17_SNN/Code/simulation/training.py:35\u001b[0m, in \u001b[0;36mtrain_nn\u001b[0;34m(nn, train_loader, valid_loader, lossfunction, optimizer, UUID)\u001b[0m\n\u001b[1;32m     32\u001b[0m     total_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m x_train\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     34\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 35\u001b[0m     L_train\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     36\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m \u001b[39m# Calculate the weighted mean loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# seed  = int(sys.argv[1])\n",
    "# model_idx = int(sys.argv[2])\n",
    "seed = 0\n",
    "model_idx = 0\n",
    "lr = -4\n",
    "\n",
    "for lr in range(-3,-7,-1):\n",
    "    exp_setup = f'{models[model_idx]}_lr_{lr}_seed_{seed}'\n",
    "    print(f'The experiment setup is {exp_setup}.')\n",
    "\n",
    "    if os.path.exists(f'./NNs/predictor_{exp_setup}'):\n",
    "        pass\n",
    "    else:\n",
    "        data = torch.load(f'./data/dataset.ds')\n",
    "\n",
    "        X_train = data['X_train'].to\n",
    "        Y_train = data['Y_train']\n",
    "        X_valid = data['X_valid']\n",
    "        Y_valid = data['Y_valid']\n",
    "        X_test  = data['X_test']\n",
    "        Y_test  = data['Y_test']\n",
    "\n",
    "        train_data = TensorDataset(X_train, Y_train)\n",
    "        valid_data = TensorDataset(X_valid, Y_valid)\n",
    "        test_data  = TensorDataset(X_test, Y_test)\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=256)\n",
    "        valid_loader = DataLoader(valid_data, batch_size=len(valid_data))\n",
    "        test_loader  = DataLoader(test_data, batch_size=len(test_data))\n",
    "\n",
    "        config.SetSeed(seed)\n",
    "\n",
    "        model_config = MyTransformer.GPT.get_default_config()\n",
    "        model_config.model_type = 'gpt-nano'\n",
    "        model_config.block_size = X_train.shape[1]\n",
    "        model = MyTransformer.GPT(model_config)\n",
    "\n",
    "        lossfunction = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=10**lr)\n",
    "\n",
    "        model, train_loss, valid_loss = training.train_nn(model, train_loader, valid_loader, lossfunction, optimizer, UUID=exp_setup)\n",
    "        torch.save(model, f'./NNs/predictor_{exp_setup}')\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(train_loss, label='train')\n",
    "        plt.plot(valid_loss, label='valid')\n",
    "        plt.savefig(f'./NNs/train_curve_{exp_setup}.pdf', format='pdf', bbox_inches='tight')\n",
    "        plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
