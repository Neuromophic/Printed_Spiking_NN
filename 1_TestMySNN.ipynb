{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.join(os.getcwd(), 'utils'))\n",
    "from configuration import *\n",
    "import torch\n",
    "import pprint\n",
    "import PrintedSpikingNN as pSNN\n",
    "from utils import *\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.projectname = 'test111'\n",
    "args.DATASET = 0\n",
    "args.DEVICE = 'cpu'\n",
    "args = FormulateArgs(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network on device: cpu.\n",
      "{'N_class': 2,\n",
      " 'N_feature': 6,\n",
      " 'N_test': 25,\n",
      " 'N_time': 300,\n",
      " 'N_train': 70,\n",
      " 'N_valid': 23,\n",
      " 'dataname': 'acuteinflammation'}\n"
     ]
    }
   ],
   "source": [
    "print(f'Training network on device: {args.DEVICE}.')\n",
    "MakeFolder(args)\n",
    "\n",
    "train_loader, datainfo = GetDataLoader(args, 'train')\n",
    "valid_loader, datainfo = GetDataLoader(args, 'valid')\n",
    "test_loader, datainfo = GetDataLoader(args, 'test')\n",
    "pprint.pprint(datainfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup: model_pSNN_data_acuteinflammation_seed_00.model.\n"
     ]
    }
   ],
   "source": [
    "SetSeed(args.SEED)\n",
    "\n",
    "setup = f\"model_pSNN_data_{datainfo['dataname']}_seed_{args.SEED:02d}.model\"\n",
    "print(f'Training setup: {setup}.')\n",
    "\n",
    "msglogger = GetMessageLogger(args, setup)\n",
    "msglogger.info(f'Training network on device: {args.DEVICE}.')\n",
    "msglogger.info(f'Training setup: {setup}.')\n",
    "msglogger.info(datainfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(f'{args.savepath}/{setup}'):\n",
    "        print(f'{setup} exists, skip this training.')\n",
    "        msglogger.info('Training was already finished.')\n",
    "else:\n",
    "    topology = [datainfo['N_feature']] + args.hidden + [datainfo['N_class']]\n",
    "    msglogger.info(f'Topology of the network: {topology}.')\n",
    "\n",
    "    psnn = pSNN.PrintedSpikingNeuralNetwork(topology, args).to(args.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "lossfunction = pSNN.LFLoss(args).to(args.DEVICE)\n",
    "optimizer = torch.optim.Adam(psnn.GetParam(), lr=args.LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haibinzhao/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch:      0 | Train loss: 6.9315e-01 | Valid loss: 6.9314e-01 | Train acc: 0.4714 | Valid acc: 0.6087 | patience:   0 | lr: 0.1 | Epoch time: 13.6 | Power: 4.17e-06 |\n",
      "| Epoch:      1 | Train loss: 6.9315e-01 | Valid loss: 6.9314e-01 | Train acc: 0.5286 | Valid acc: 0.6087 | patience:   0 | lr: 0.1 | Epoch time: 13.9 | Power: 5.36e-06 |\n",
      "| Epoch:      2 | Train loss: 6.9315e-01 | Valid loss: 6.9314e-01 | Train acc: 0.5286 | Valid acc: 0.6087 | patience:   0 | lr: 0.1 | Epoch time: 13.7 | Power: 2.00e-05 |\n",
      "| Epoch:      3 | Train loss: 6.9314e-01 | Valid loss: 6.9313e-01 | Train acc: 0.5286 | Valid acc: 0.6087 | patience:   0 | lr: 0.1 | Epoch time: 14.3 | Power: 2.03e-05 |\n",
      "| Epoch:      4 | Train loss: 6.9314e-01 | Valid loss: 6.9313e-01 | Train acc: 0.5286 | Valid acc: 0.6087 | patience:   0 | lr: 0.1 | Epoch time: 14.0 | Power: 3.72e-05 |\n",
      "| Epoch:      5 | Train loss: 6.9314e-01 | Valid loss: 6.9312e-01 | Train acc: 0.5286 | Valid acc: 0.6087 | patience:   0 | lr: 0.1 | Epoch time: 14.0 | Power: 9.93e-06 |\n",
      "| Epoch:      6 | Train loss: 6.9314e-01 | Valid loss: 6.9312e-01 | Train acc: 0.5286 | Valid acc: 0.6087 | patience:   0 | lr: 0.1 | Epoch time: 13.6 | Power: 2.34e-05 |\n",
      "| Epoch:      7 | Train loss: 6.9314e-01 | Valid loss: 6.9311e-01 | Train acc: 0.5286 | Valid acc: 0.6087 | patience:   0 | lr: 0.1 | Epoch time: 13.4 | Power: 7.89e-06 |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/haibinzhao/Desktop/TECO/17_SNN/Code/1_TestMySNN.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/haibinzhao/Desktop/TECO/17_SNN/Code/1_TestMySNN.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mPROGRESSIVE:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/haibinzhao/Desktop/TECO/17_SNN/Code/1_TestMySNN.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     psnn, best \u001b[39m=\u001b[39m train_pnn_progressive(psnn, train_loader, valid_loader, lossfunction, optimizer, args, msglogger, UUID\u001b[39m=\u001b[39;49msetup)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/haibinzhao/Desktop/TECO/17_SNN/Code/1_TestMySNN.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/haibinzhao/Desktop/TECO/17_SNN/Code/1_TestMySNN.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     psnn, best \u001b[39m=\u001b[39m train_pnn(psnn, train_loader, valid_loader, lossfunction, optimizer, args, msglogger, UUID\u001b[39m=\u001b[39msetup)\n",
      "File \u001b[0;32m~/Desktop/TECO/17_SNN/Code/utils/training.py:114\u001b[0m, in \u001b[0;36mtrain_pnn_progressive\u001b[0;34m(nn, train_loader, valid_loader, lossfunction, optimizer, args, logger, UUID)\u001b[0m\n\u001b[1;32m    111\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhyperparameters in printed neural network for training :\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mepoch : \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m-6d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m |\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    113\u001b[0m L_train \u001b[39m=\u001b[39m lossfunction(nn, x_train, y_train)\n\u001b[0;32m--> 114\u001b[0m train_acc, train_power \u001b[39m=\u001b[39m evaluator(nn, x_train, y_train)\n\u001b[1;32m    115\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    116\u001b[0m L_train\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/TECO/17_SNN/Code/utils/evaluation.py:55\u001b[0m, in \u001b[0;36mEvaluator.forward\u001b[0;34m(self, nn, x, label)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmetric \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnominal_snn\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     54\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperformance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnominal_snn\n\u001b[0;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperformance(nn, x, label)\n",
      "File \u001b[0;32m~/Desktop/TECO/17_SNN/Code/utils/evaluation.py:30\u001b[0m, in \u001b[0;36mEvaluator.nominal_temporal\u001b[0;34m(self, nn, x, label)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnominal_temporal\u001b[39m(\u001b[39mself\u001b[39m, nn, x, label): \u001b[39m# time series data & time series model\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     output \u001b[39m=\u001b[39m nn(x)\n\u001b[1;32m     31\u001b[0m     act, idx \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msum(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m     corrects \u001b[39m=\u001b[39m (label\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m idx)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/TECO/17_SNN/Code/PrintedSpikingNN.py:184\u001b[0m, in \u001b[0;36mPrintedSpikingNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/TECO/17_SNN/Code/PrintedSpikingNN.py:125\u001b[0m, in \u001b[0;36mpLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m z_new \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(result, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpower \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpower \u001b[39m/\u001b[39m T\n\u001b[0;32m--> 125\u001b[0m a_new \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSG(z_new)\n\u001b[1;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m a_new\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/TECO/17_SNN/Code/PrintedSpikingNN.py:48\u001b[0m, in \u001b[0;36mSGLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSG_Group)):\n\u001b[1;32m     47\u001b[0m     x_temp \u001b[39m=\u001b[39m x[:, n, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     result\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSG_Group[n](x_temp))\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(result)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/TECO/17_SNN/Code/PrintedSpikingNN.py:23\u001b[0m, in \u001b[0;36mpSpikeGenerator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspike_generator(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/TECO/17_SNN/Code/utils/MyTransformer.py:195\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    193\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mdrop(tok_emb \u001b[39m+\u001b[39m pos_emb)\n\u001b[1;32m    194\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mh:\n\u001b[0;32m--> 195\u001b[0m     x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m    196\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mln_f(x)\n\u001b[1;32m    198\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregression_head(x)\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/TECO/17_SNN/Code/utils/MyTransformer.py:80\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     79\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(x))\n\u001b[0;32m---> 80\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_forward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(x))\n\u001b[1;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Desktop/TECO/17_SNN/Code/utils/MyTransformer.py:72\u001b[0m, in \u001b[0;36mBlock.mlp_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmlp_forward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     71\u001b[0m     m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp\n\u001b[0;32m---> 72\u001b[0m     x \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39;49mc_fc(x)\n\u001b[1;32m     73\u001b[0m     x \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mact(x)\n\u001b[1;32m     74\u001b[0m     x \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mc_proj(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if args.PROGRESSIVE:\n",
    "    psnn, best = train_pnn_progressive(psnn, train_loader, valid_loader, lossfunction, optimizer, args, msglogger, UUID=setup)\n",
    "else:\n",
    "    psnn, best = train_pnn(psnn, train_loader, valid_loader, lossfunction, optimizer, args, msglogger, UUID=setup)\n",
    "\n",
    "if best:\n",
    "    if not os.path.exists(f'{args.savepath}/'):\n",
    "        os.makedirs(f'{args.savepath}/')\n",
    "    torch.save(psnn, f'{args.savepath}/{setup}')\n",
    "    msglogger.info('Training if finished.')\n",
    "else:\n",
    "    msglogger.warning('Time out, further training is necessary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "c = 0\n",
    "plt.figure(figsize=(30,5))\n",
    "plt.plot(X_train[n,c,:]/10, label='input')\n",
    "plt.plot(spikes[n,c,:].detach().numpy(), label='spike')\n",
    "plt.plot(memories[n,c,:].detach().numpy(), label='memory')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmac = psnn.TemporalWeightedSum(N_channel, N_class)\n",
    "tmac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mactmac = tmac(spikes)\n",
    "mactmac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "topology = [N_channel, 3, N_class]\n",
    "snn = psnn.SpikingNeuralNetwork(topology, beta, threshold)\n",
    "optimizer = torch.optim.Adam(snn.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn.ResetOutput(False, 1)\n",
    "input = snn.model[0](X_train)\n",
    "spikes,memories = snn.model[1](input)\n",
    "n = 0\n",
    "c = 0\n",
    "plt.figure(figsize=(30,5))\n",
    "plt.plot(input[n,c,:].detach().numpy(), label='input')\n",
    "plt.plot(spikes[n,c,:].detach().numpy(), label='spike')\n",
    "plt.plot(memories[n,c,:].detach().numpy(), label='memory')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "topology = [N_channel, 3, N_class]\n",
    "snn = psnn.SpikingNeuralNetwork(topology, beta=torch.zeros(1), threshold=torch.ones(1))\n",
    "optimizer = torch.optim.Adam(snn.parameters(), lr=0.1)\n",
    "loss_fn = psnn.SNNLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000):\n",
    "    y_pred_train = snn(X_train)\n",
    "    loss_train = loss_fn(y_pred_train, y_train)\n",
    "    acc_train = (y_pred_train.sum(2).argmax(dim=1) == y_train).float().mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    print(f'epoch: {epoch:-8d} | train loss: {loss_train:.5e} | train acc: {acc_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randperm(N_train)[:100]\n",
    "\n",
    "X_visual = X_train[idx,:,:]\n",
    "y_visual = y_train[idx]\n",
    "\n",
    "plt.figure(figsize=(30,5))\n",
    "for i in range(100):\n",
    "    plt.subplot(10,10,i+1)\n",
    "    plt.plot(X_visual[i,0,:].detach().numpy(), label=f'{y_visual[i]}')\n",
    "    plt.axis('off')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
