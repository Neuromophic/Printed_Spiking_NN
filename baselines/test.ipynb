{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from pprint import pprint\n",
    "import math\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "import training as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = sys.argv[1]\n",
    "ds_idx = sys.argv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "ds_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: ArrowHead, N_train: 126, N_valid: 42, N_test: 43, N_class: 3, N_feature: 251, N_channel: 1, N_length: 251\n",
      "X_train: torch.Size([126, 1, 251]), X_valid: torch.Size([42, 1, 251]), X_test: torch.Size([43, 1, 251])\n"
     ]
    }
   ],
   "source": [
    "datasets = os.listdir('../ts_datasets/')\n",
    "datasets = [dataset for dataset in datasets if dataset.endswith('.tsds')]\n",
    "datasets.sort()\n",
    "dataset = datasets[ds_idx]\n",
    "package = torch.load(f'../ts_datasets/{dataset}')\n",
    "\n",
    "name = package['name']\n",
    "\n",
    "N_train = package['N_train']\n",
    "N_valid = package['N_valid']\n",
    "N_test = package['N_test']\n",
    "\n",
    "N_class = package['N_class']\n",
    "\n",
    "N_channel = package['N_channel']\n",
    "N_length = package['N_length']\n",
    "\n",
    "N_feature = N_channel * N_length\n",
    "\n",
    "print(f'dataset: {name}, N_train: {N_train}, N_valid: {N_valid}, N_test: {N_test}, N_class: {N_class}, N_feature: {N_feature}, N_channel: {N_channel}, N_length: {N_length}')\n",
    "\n",
    "X_train = package['X_train']\n",
    "X_valid = package['X_valid']\n",
    "X_test = package['X_test']\n",
    "\n",
    "y_train = package['Y_train']\n",
    "y_valid = package['Y_valid']\n",
    "y_test = package['Y_test']\n",
    "\n",
    "print(f'X_train: {X_train.shape}, X_valid: {X_valid.shape}, X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'X_train', 'Y_train', 'X_valid', 'Y_valid', 'X_test', 'Y_test', 'N_train', 'N_valid', 'N_test', 'N_channel', 'N_length', 'N_class'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stride = max(2,int(N_class/2))\n",
    "size = N_class\n",
    "padding = int((N_class-1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[126, 63, 32, 16, 8, 4]\n"
     ]
    }
   ],
   "source": [
    "L_in = N_length\n",
    "L_outs = []\n",
    "while L_in > N_class:\n",
    "    L_out = int((L_in+2*padding-size) / stride) + 1\n",
    "    L_outs.append(L_out)\n",
    "    L_in = L_out\n",
    "L_outs.pop()\n",
    "N_channels = torch.linspace(N_channel, 1, len(L_outs)+1).round().long()\n",
    "\n",
    "print(L_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0_conv): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (0_relu): PReLU(num_parameters=1)\n",
      "  (1_conv): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (1_relu): PReLU(num_parameters=1)\n",
      "  (2_conv): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (2_relu): PReLU(num_parameters=1)\n",
      "  (3_conv): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (3_relu): PReLU(num_parameters=1)\n",
      "  (4_conv): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (4_relu): PReLU(num_parameters=1)\n",
      "  (5_conv): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (5_relu): PReLU(num_parameters=1)\n",
      "  (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  (7): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (8): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haibinzhao/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/conv.py:132: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "model = torch.nn.Sequential()\n",
    "for i in range(len(L_outs)):\n",
    "    model.add_module(f'{i}_conv', torch.nn.Conv1d(N_channels[i], N_channels[i+1], N_class, stride=stride, padding=padding))\n",
    "    model.add_module(f'{i}_relu', torch.nn.PReLU())\n",
    "model.add_module(f'{i+1}', torch.nn.Flatten())\n",
    "model.add_module(f'{i+2}', torch.nn.Linear(L_outs[-1], N_class))\n",
    "model.add_module(f'{i+3}', torch.nn.Softmax(dim=1))\n",
    "pprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setup = f'{name}_{seed}.cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:        0 | train loss: 1.10873e+00 | valid loss: 1.12744e+00 | train acc: 0.3175 | valid acc: 0.2381 | test acc: 0.3488 | patience: 0\n",
      "epoch:      100 | train loss: 1.10740e+00 | valid loss: 1.12459e+00 | train acc: 0.3175 | valid acc: 0.2381 | test acc: 0.3488 | patience: 0\n",
      "epoch:      200 | train loss: 1.10619e+00 | valid loss: 1.12186e+00 | train acc: 0.3175 | valid acc: 0.2381 | test acc: 0.3488 | patience: 0\n",
      "epoch:      300 | train loss: 1.10496e+00 | valid loss: 1.11900e+00 | train acc: 0.3175 | valid acc: 0.2381 | test acc: 0.3488 | patience: 0\n",
      "epoch:      400 | train loss: 1.10373e+00 | valid loss: 1.11604e+00 | train acc: 0.3175 | valid acc: 0.2381 | test acc: 0.3488 | patience: 0\n",
      "epoch:      500 | train loss: 1.10253e+00 | valid loss: 1.11304e+00 | train acc: 0.3175 | valid acc: 0.2381 | test acc: 0.3488 | patience: 0\n",
      "epoch:      600 | train loss: 1.10140e+00 | valid loss: 1.11005e+00 | train acc: 0.3175 | valid acc: 0.2381 | test acc: 0.3488 | patience: 0\n",
      "epoch:      700 | train loss: 1.10034e+00 | valid loss: 1.10713e+00 | train acc: 0.3175 | valid acc: 0.2381 | test acc: 0.3488 | patience: 0\n",
      "epoch:      800 | train loss: 1.09939e+00 | valid loss: 1.10430e+00 | train acc: 0.3175 | valid acc: 0.2381 | test acc: 0.3488 | patience: 0\n",
      "epoch:      900 | train loss: 1.09854e+00 | valid loss: 1.10161e+00 | train acc: 0.3175 | valid acc: 0.2381 | test acc: 0.3488 | patience: 0\n",
      "epoch:     1000 | train loss: 1.09780e+00 | valid loss: 1.09908e+00 | train acc: 0.3175 | valid acc: 0.2381 | test acc: 0.3488 | patience: 0\n",
      "epoch:     1100 | train loss: 1.09718e+00 | valid loss: 1.09673e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     1200 | train loss: 1.09666e+00 | valid loss: 1.09459e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     1300 | train loss: 1.09625e+00 | valid loss: 1.09266e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     1400 | train loss: 1.09593e+00 | valid loss: 1.09095e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     1500 | train loss: 1.09568e+00 | valid loss: 1.08946e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     1600 | train loss: 1.09550e+00 | valid loss: 1.08816e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     1700 | train loss: 1.09536e+00 | valid loss: 1.08705e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     1800 | train loss: 1.09525e+00 | valid loss: 1.08611e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     1900 | train loss: 1.09516e+00 | valid loss: 1.08531e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     2000 | train loss: 1.09424e+00 | valid loss: 1.08340e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     2100 | train loss: 1.09312e+00 | valid loss: 1.08172e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     2200 | train loss: 1.09106e+00 | valid loss: 1.07931e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     2300 | train loss: 1.08740e+00 | valid loss: 1.07552e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     2400 | train loss: 1.08123e+00 | valid loss: 1.06924e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     2500 | train loss: 1.07135e+00 | valid loss: 1.05911e+00 | train acc: 0.3730 | valid acc: 0.4524 | test acc: 0.3488 | patience: 0\n",
      "epoch:     2600 | train loss: 1.05798e+00 | valid loss: 1.04292e+00 | train acc: 0.3810 | valid acc: 0.4524 | test acc: 0.3721 | patience: 0\n",
      "epoch:     2700 | train loss: 1.03995e+00 | valid loss: 1.02282e+00 | train acc: 0.4365 | valid acc: 0.4524 | test acc: 0.4186 | patience: 0\n",
      "epoch:     2800 | train loss: 1.02221e+00 | valid loss: 1.00095e+00 | train acc: 0.4603 | valid acc: 0.5000 | test acc: 0.4419 | patience: 0\n",
      "epoch:     2900 | train loss: 1.00604e+00 | valid loss: 9.82503e-01 | train acc: 0.4921 | valid acc: 0.5476 | test acc: 0.4884 | patience: 0\n",
      "epoch:     3000 | train loss: 9.91181e-01 | valid loss: 9.66286e-01 | train acc: 0.5397 | valid acc: 0.5952 | test acc: 0.5581 | patience: 0\n",
      "epoch:     3100 | train loss: 9.77085e-01 | valid loss: 9.50912e-01 | train acc: 0.5714 | valid acc: 0.6190 | test acc: 0.6279 | patience: 0\n",
      "epoch:     3200 | train loss: 9.63994e-01 | valid loss: 9.35975e-01 | train acc: 0.5714 | valid acc: 0.6429 | test acc: 0.6279 | patience: 0\n",
      "epoch:     3300 | train loss: 9.51322e-01 | valid loss: 9.20717e-01 | train acc: 0.6032 | valid acc: 0.6190 | test acc: 0.6512 | patience: 0\n",
      "epoch:     3400 | train loss: 9.38913e-01 | valid loss: 9.06017e-01 | train acc: 0.6111 | valid acc: 0.7143 | test acc: 0.7442 | patience: 0\n",
      "epoch:     3500 | train loss: 9.26752e-01 | valid loss: 8.91033e-01 | train acc: 0.6587 | valid acc: 0.7381 | test acc: 0.7442 | patience: 0\n",
      "epoch:     3600 | train loss: 9.15053e-01 | valid loss: 8.76344e-01 | train acc: 0.6746 | valid acc: 0.7619 | test acc: 0.7674 | patience: 0\n",
      "epoch:     3700 | train loss: 9.04071e-01 | valid loss: 8.61544e-01 | train acc: 0.6984 | valid acc: 0.8095 | test acc: 0.7442 | patience: 0\n",
      "epoch:     3800 | train loss: 8.93990e-01 | valid loss: 8.48147e-01 | train acc: 0.6984 | valid acc: 0.8333 | test acc: 0.7442 | patience: 0\n",
      "epoch:     3900 | train loss: 8.84898e-01 | valid loss: 8.35513e-01 | train acc: 0.7063 | valid acc: 0.8571 | test acc: 0.7442 | patience: 0\n",
      "epoch:     4000 | train loss: 8.76908e-01 | valid loss: 8.23309e-01 | train acc: 0.6905 | valid acc: 0.8571 | test acc: 0.7442 | patience: 0\n",
      "epoch:     4100 | train loss: 8.69870e-01 | valid loss: 8.12025e-01 | train acc: 0.6905 | valid acc: 0.8571 | test acc: 0.7442 | patience: 0\n",
      "epoch:     4200 | train loss: 8.63713e-01 | valid loss: 8.01717e-01 | train acc: 0.6905 | valid acc: 0.8571 | test acc: 0.7442 | patience: 0\n",
      "epoch:     4300 | train loss: 8.58323e-01 | valid loss: 7.92637e-01 | train acc: 0.6905 | valid acc: 0.8571 | test acc: 0.7442 | patience: 0\n",
      "epoch:     4400 | train loss: 8.53374e-01 | valid loss: 7.84619e-01 | train acc: 0.6984 | valid acc: 0.8571 | test acc: 0.7442 | patience: 0\n",
      "epoch:     4500 | train loss: 8.48523e-01 | valid loss: 7.77942e-01 | train acc: 0.7063 | valid acc: 0.8571 | test acc: 0.7442 | patience: 1\n",
      "epoch:     4600 | train loss: 8.43874e-01 | valid loss: 7.71994e-01 | train acc: 0.7143 | valid acc: 0.8571 | test acc: 0.7442 | patience: 0\n",
      "epoch:     4700 | train loss: 8.39760e-01 | valid loss: 7.66364e-01 | train acc: 0.7143 | valid acc: 0.8571 | test acc: 0.7442 | patience: 0\n",
      "epoch:     4800 | train loss: 8.36292e-01 | valid loss: 7.61058e-01 | train acc: 0.7222 | valid acc: 0.8571 | test acc: 0.7442 | patience: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/haibinzhao/Desktop/TECO/16_SNN/Code/baselines/test.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/haibinzhao/Desktop/TECO/16_SNN/Code/baselines/test.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m best_nn \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39;49mtraining(model, loss_fn, optimizer, X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
      "File \u001b[0;32m~/Desktop/TECO/16_SNN/Code/baselines/training.py:14\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(nn, loss_fn, optimizer, X_train, y_train, X_valid, y_valid, X_test, y_test)\u001b[0m\n\u001b[1;32m     12\u001b[0m acc_train \u001b[39m=\u001b[39m (y_pred_train\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m y_train)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m loss_train\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_nn = T.training(model, loss_fn, optimizer, X_train, y_train, X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'AtrialFibrillation', Sequential(\n",
      "  (0_conv): Conv1d(2, 2, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (0_relu): Tanh()\n",
      "  (1_conv): Conv1d(2, 2, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (1_relu): Tanh()\n",
      "  (2_conv): Conv1d(2, 2, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (2_relu): Tanh()\n",
      "  (3_conv): Conv1d(2, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (3_relu): Tanh()\n",
      "  (4_conv): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (4_relu): Tanh()\n",
      "  (5_conv): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (5_relu): Tanh()\n",
      "  (6_conv): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (6_relu): Tanh()\n",
      "  (7): Flatten(start_dim=1, end_dim=-1)\n",
      "  (8): Linear(in_features=5, out_features=3, bias=True)\n",
      "  (9): Softmax(dim=1)\n",
      "): Sequential(\n",
      "  (0_conv): Conv1d(2, 2, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (0_relu): Tanh()\n",
      "  (1_conv): Conv1d(2, 2, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (1_relu): Tanh()\n",
      "  (2_conv): Conv1d(2, 2, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (2_relu): Tanh()\n",
      "  (3_conv): Conv1d(2, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (3_relu): Tanh()\n",
      "  (4_conv): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (4_relu): Tanh()\n",
      "  (5_conv): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (5_relu): Tanh()\n",
      "  (6_conv): Conv1d(1, 1, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (6_relu): Tanh()\n",
      "  (7): Flatten(start_dim=1, end_dim=-1)\n",
      "  (8): Linear(in_features=5, out_features=3, bias=True)\n",
      "  (9): Softmax(dim=1)\n",
      "), 'acc_train': tensor(0.4000), 'acc_valid': tensor(0.2000), 'acc_test': tensor(0.3333)}\n"
     ]
    }
   ],
   "source": [
    "acc_train = (best_nn(X_train).argmax(dim=1) == y_train).float().mean()\n",
    "acc_valid = (best_nn(X_valid).argmax(dim=1) == y_valid).float().mean()\n",
    "acc_test = (best_nn(X_test).argmax(dim=1) == y_test).float().mean()\n",
    "\n",
    "package = {'name': name, model: best_nn, 'acc_train': acc_train, 'acc_valid': acc_valid, 'acc_test': acc_test}\n",
    "print(package)\n",
    "\n",
    "if not os.path.exists('./baseline_result/CNN/'):\n",
    "    os.makedirs('./baseline_result/CNN/')\n",
    "torch.save(package, f'./baseline_result/CNN/{setup}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
